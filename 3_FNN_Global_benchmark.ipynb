{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8d2fb9",
   "metadata": {},
   "source": [
    "# FNN- Global benchmark\n",
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207956e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from numpy import array, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a179776",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92eda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory = 'C:/.../TFT_for_Stock_Movement_Prediction/data'\n",
    "\n",
    "# Target and return feature\n",
    "CCR = pd.read_csv(os.path.join(Directory, 'CCR.csv'), index_col = [0])\n",
    "\n",
    "### Features\n",
    "## Time features - Categorical\n",
    "time_features = pd.read_csv(os.path.join(Directory, 'time_features.csv'), index_col = [0])['0'].tolist()\n",
    "for i in range(len(time_features)):\n",
    "    locals()[time_features[i]] = pd.read_csv(os.path.join(Directory, time_features[i] + '.csv'), index_col = [0])\n",
    "\n",
    "## Basic historical features\n",
    "bh_features = pd.read_csv(os.path.join(Directory, 'bh_features.csv'), index_col = [0])['0'].tolist()\n",
    "for i in range(len(bh_features)):\n",
    "    locals()[bh_features[i]] = pd.read_csv(os.path.join(Directory, bh_features[i] + '.csv'), index_col = [0])\n",
    "\n",
    "# Categorical\n",
    "bh_categorical_features = pd.read_csv(os.path.join(Directory, 'bh_categorical_features.csv'), index_col = [0])['0'].tolist()\n",
    "\n",
    "# Continuous\n",
    "bh_continuous_features = pd.read_csv(os.path.join(Directory, 'bh_continuous_features.csv'), index_col = [0])['0'].tolist()\n",
    "\n",
    "## Technical indicators - Continuous\n",
    "indicator_features = pd.read_csv(os.path.join(Directory, 'indicator_features.csv'), index_col = [0])['0'].tolist()\n",
    "for i in range(len(indicator_features)):\n",
    "    locals()[indicator_features[i]] = pd.read_csv(os.path.join(Directory, indicator_features[i] + '.csv'), index_col = [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d4de9",
   "metadata": {},
   "source": [
    "### Model preparation\n",
    "#### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study periods length\n",
    "period_b = 0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250\n",
    "period_e = 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250\n",
    "\n",
    "# Split period into training, validation and test set\n",
    "training_size = 750\n",
    "test_size = 250\n",
    "validation_split = 0.2\n",
    "\n",
    "# Target\n",
    "Target_feature = ['CCR']\n",
    "\n",
    "# Features\n",
    "Feature_type = ['int', 'float']\n",
    "time_varying_known_categoricals = time_features\n",
    "time_varying_unknown_categoricals = bh_categorical_features\n",
    "time_varying_unknown_reals = bh_continuous_features + indicator_features\n",
    "time_varying_reals_to_scale = time_varying_unknown_reals + Target_feature\n",
    "\n",
    "# Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Model parameters\n",
    "max_encoder_length = 258\n",
    "batch_size = 128\n",
    "max_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 50\n",
    "dropout = 0.20 \n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "loss = 'mae'\n",
    "\n",
    "# Early stopping\n",
    "mode = 'min'\n",
    "patience = 10\n",
    "min_delta = 1e-4\n",
    "cb = EarlyStopping(mode = mode, patience = patience, min_delta = min_delta)\n",
    "\n",
    "# File path to save results\n",
    "File_name_results = 'results/FNN/Results_FNN.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc198d37",
   "metadata": {},
   "source": [
    "#### Preparation of datasets / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc55cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for each study period and stock\n",
    "def dataset(period, stock):\n",
    "    \n",
    "    global scaler\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = pd.DataFrame(index = globals()[Target_feature[0]].index[period_b[period] : period_e[period]])\n",
    "    \n",
    "    ## Add features\n",
    "    # Target feature\n",
    "    data['CCR'] = globals()[Target_feature[0]][[globals()[Target_feature[0]].columns[stock]]][period_b[period] : period_e[period]]\n",
    "    \n",
    "    # Time varying known categoricals\n",
    "    for f in range(len(time_varying_known_categoricals)):\n",
    "        data[time_varying_known_categoricals[f]] = globals()[time_varying_known_categoricals[f]].astype(Feature_type[0])\n",
    "\n",
    "    # Time varying unknown categoricals\n",
    "    for f in range(len(time_varying_unknown_categoricals)):\n",
    "        data[time_varying_unknown_categoricals[f]] = globals()[time_varying_unknown_categoricals[f]][globals()[time_varying_unknown_categoricals[f]].columns[stock]].astype(Feature_type[0])\n",
    "    \n",
    "    # Time varying unknown reals\n",
    "    for f in range(len(time_varying_unknown_reals)):\n",
    "        data[time_varying_unknown_reals[f]] = globals()[time_varying_unknown_reals[f]][globals()[time_varying_unknown_reals[f]].columns[stock]].astype(Feature_type[1])\n",
    "\n",
    "    ## Scaling\n",
    "    # Reset index\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    # Scaling real features\n",
    "    scaler = scaler.fit(data[time_varying_reals_to_scale][0 : training_size])\n",
    "    data.loc[0 : len(data) - 1, time_varying_reals_to_scale] = scaler.transform(data[time_varying_reals_to_scale][0 : len(data)])\n",
    "\n",
    "    return data\n",
    "\n",
    "def dataset_target(period, stock):\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = pd.DataFrame(index = globals()[Target_feature[0]].index[period_b[period] : period_e[period]])\n",
    "    \n",
    "    # Target\n",
    "    data['Target'] = globals()[Target_feature[0]][[globals()[Target_feature[0]].columns[stock]]][period_b[period] : period_e[period]].shift(-1).fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Sequence split\n",
    "def split_dataset(data, max_encoder_length):\n",
    "\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        idx = i + max_encoder_length\n",
    "        if idx > len(data):\n",
    "            break\n",
    "        seq_x = data[i : idx]\n",
    "        X.append(seq_x)\n",
    "        \n",
    "    return array(X)\n",
    "\n",
    "def split_target(data, max_encoder_length):\n",
    "\n",
    "    X = list()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        idx = i + max_encoder_length\n",
    "        if idx > len(data):\n",
    "            break\n",
    "        seq_x = data[idx - 1]\n",
    "        X.append(seq_x)\n",
    "        \n",
    "    return array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7af0b5",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda235b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "Results = pd.DataFrame(index = range(period_b[0], period_e[9] - training_size), columns = locals()[Target_feature[0]].columns)\n",
    "\n",
    "for i in range(len(period_b)):\n",
    "    start_period = time.time()\n",
    "    temp_dataset_list =[]\n",
    "    temp_target_list =[]\n",
    "    for j in range(len(locals()[Target_feature[0]].columns)):\n",
    "        temp_dataset = dataset(i, j)\n",
    "        temp_target = dataset_target(i, j)\n",
    "        temp_dataset_list.append(np.array(temp_dataset).reshape(len(temp_dataset), len(temp_dataset.columns)))\n",
    "        temp_target_list.append(np.array(temp_target).reshape(len(temp_target), len(temp_target.columns)))\n",
    "    dataset_stack = hstack((temp_dataset_list))   \n",
    "    target_stack = hstack((temp_target_list))\n",
    "    X = split_dataset(dataset_stack[ : training_size], max_encoder_length)\n",
    "    y = split_target(target_stack[ : training_size], max_encoder_length)\n",
    "    X_test = split_dataset(dataset_stack[training_size - max_encoder_length + 1 : ], max_encoder_length)\n",
    "    X_shape = X.shape[1] * X.shape[2]\n",
    "    X = X.reshape((X.shape[0], X_shape))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_shape))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, activation = 'relu', input_dim = X_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_size, activation = 'relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_size, activation = 'relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(locals()[Target_feature[0]].columns)))\n",
    "    model.compile(optimizer = optimizer, loss = loss)\n",
    "    model.fit(X, y, batch_size = batch_size, validation_split = validation_split, epochs = max_epochs, verbose = 0, callbacks = cb)\n",
    "    results_temp = model.predict(X_test, verbose = 0)\n",
    "    Results[period_b[i] : period_b[i] + test_size] = results_temp\n",
    "    print(f'Compilation time - Period {i + 1}: {round(time.time() - start_period)} seconds')\n",
    "Results.to_csv(File_name_results)\n",
    "print(f'Compilation time: {round(time.time() - start)} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
